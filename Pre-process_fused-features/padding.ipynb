{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9838,
     "status": "ok",
     "timestamp": 1735027267914,
     "user": {
      "displayName": "Deepfake Detection",
      "userId": "16853363008574283880"
     },
     "user_tz": -330
    },
    "id": "3Bt9zHS5OmmC",
    "outputId": "c0cd5d77-a815-4713-b5a4-c157bc5a6877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured GPU with a memory limit of 13 000 MB.\n",
      "Mixed precision enabled for speedup.\n",
      "Found 2 features in the input file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Keys:   0%|          | 0/2 [00:00<?, ?key/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features with parallelism...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Processing:   0%|          | 0/16 [00:00<?, ?batch/s]\u001b[A\n",
      "Feature Processing: 100%|██████████| 16/16 [00:00<00:00, 130.28batch/s]\n",
      "Processing Keys:  50%|█████     | 1/2 [00:01<00:01,  1.95s/key]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features with parallelism...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Processing: 100%|██████████| 16/16 [00:00<00:00, 124506.24batch/s]\n",
      "Processing Keys: 100%|██████████| 2/2 [00:03<00:00,  1.95s/key]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "input_file_path = \"drive/MyDrive/final_dataset/features/fused/fused_features_2.h5\"\n",
    "output_file_path = \"drive/MyDrive/final_dataset/features/fused/fused_features_padded_2.h5\"\n",
    "\n",
    "TARGET_LENGTH = 48\n",
    "PAD_VALUE = 0.0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                device, [tf.config.LogicalDeviceConfiguration(memory_limit=13000)]\n",
    "            )\n",
    "            print(\"Configured GPU with a memory limit of 13 000 MB.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n",
    "\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    print(\"Mixed precision enabled for speedup.\")\n",
    "except ValueError:\n",
    "    print(\"Mixed precision not supported, running with default precision.\")\n",
    "\n",
    "def pad_sequence(sequence, target_length, pad_value):\n",
    "    current_length = sequence.shape[0]\n",
    "    if current_length >= target_length:\n",
    "        return sequence[:target_length]\n",
    "    else:\n",
    "        padding = np.full((target_length - current_length, sequence.shape[1]), pad_value)\n",
    "        return np.vstack([sequence, padding])\n",
    "\n",
    "def process_features_in_parallel(features, batch_size):\n",
    "    print(\"Processing features with parallelism...\")\n",
    "    processed_batches = []\n",
    "\n",
    "    def process_batch(start, end):\n",
    "        batch = features[start:end]\n",
    "        padded_batch = [pad_sequence(f, TARGET_LENGTH, PAD_VALUE) for f in batch]\n",
    "        return padded_batch\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_batch, i, min(i + batch_size, len(features)))\n",
    "            for i in range(0, len(features), batch_size)\n",
    "        ]\n",
    "        for future in tqdm(futures, desc=\"Feature Processing\", unit=\"batch\"):\n",
    "            processed_batches.extend(future.result())\n",
    "\n",
    "    return np.array(processed_batches)\n",
    "\n",
    "def process_h5_file(input_file, output_file, batch_size):\n",
    "    with h5py.File(input_file, \"r\") as input_h5, h5py.File(output_file, \"w\") as output_h5:\n",
    "        keys = list(input_h5.keys())\n",
    "        print(f\"Found {len(keys)} features in the input file.\")\n",
    "\n",
    "        for key in tqdm(keys, desc=\"Processing Keys\", unit=\"key\"):\n",
    "            features = input_h5[key][:]\n",
    "            labels = np.zeros(len(features))  \n",
    "\n",
    "            padded_features = process_features_in_parallel(features, batch_size)\n",
    "\n",
    "            output_h5.create_dataset(key, data=padded_features)\n",
    "\n",
    "process_h5_file(\n",
    "    input_file=input_file_path,\n",
    "    output_file=output_file_path,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3154,
     "status": "ok",
     "timestamp": 1735027291052,
     "user": {
      "displayName": "Deepfake Detection",
      "userId": "16853363008574283880"
     },
     "user_tz": -330
    },
    "id": "QFJeK7M9wurg",
    "outputId": "ef582b21-776a-452d-b7f6-6ffd7003f8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 features in the input file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing Keys: 100%|██████████| 2/2 [00:02<00:00,  1.33s/key]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_file_path = \"drive/MyDrive/final_dataset/features/fused/fused_features_padded_2.h5\"\n",
    "output_file_path = \"drive/MyDrive/final_dataset/features/fused/fused_features_standardized_2.h5\"\n",
    "\n",
    "def standardize_features(input_file, output_file):\n",
    "    with h5py.File(input_file, \"r\") as input_h5, h5py.File(output_file, \"w\") as output_h5:\n",
    "        keys = list(input_h5.keys())\n",
    "        print(f\"Found {len(keys)} features in the input file.\")\n",
    "\n",
    "        for key in tqdm(keys, desc=\"Standardizing Keys\", unit=\"key\"):\n",
    "            features = input_h5[key][:]\n",
    "\n",
    "            mean = np.mean(features, axis=0)\n",
    "            std = np.std(features, axis=0)\n",
    "\n",
    "            std[std == 0] = 1\n",
    "            standardized_features = (features - mean) / std\n",
    "\n",
    "            output_h5.create_dataset(key, data=standardized_features)\n",
    "\n",
    "# Run the standardization\n",
    "standardize_features(input_file_path, output_file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNF6b8sUCYn+A/vCbOJ/pNx",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1uov4LYIiHGFyd71NU3ddDVVyIqNg2UnP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
