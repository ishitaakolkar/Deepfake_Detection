{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27688,
     "status": "ok",
     "timestamp": 1733911251651,
     "user": {
      "displayName": "Deepfake Detection",
      "userId": "16853363008574283880"
     },
     "user_tz": -330
    },
    "id": "R6GTyTJSKGZG",
    "outputId": "0e3feee5-b8c9-4fbf-a622-1926247fb744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2936,
     "status": "ok",
     "timestamp": 1733911290573,
     "user": {
      "displayName": "Deepfake Detection",
      "userId": "16853363008574283880"
     },
     "user_tz": -330
    },
    "id": "RIpzre7CKmj5",
    "outputId": "b8e8a41d-d0c3-46f2-f8fe-6c87cf73afc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpHxSuCNKlD9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1733911315737,
     "user": {
      "displayName": "Deepfake Detection",
      "userId": "16853363008574283880"
     },
     "user_tz": -330
    },
    "id": "b8B0wcN2KrEx",
    "outputId": "7833a07c-dfe3-4801-8346-7dfdf8bacde3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU configured with a memory limit of 4096 MB.\n",
      "Mixed precision enabled for speedup.\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "            \n",
    "            tf.config.set_logical_device_configuration(\n",
    "                device, [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  \n",
    "            )\n",
    "            print(f\"GPU configured with a memory limit of 4096 MB.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n",
    "\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled for speedup.\")\n",
    "except ValueError:\n",
    "    print(\"Mixed precision not supported, running with default precision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNJaKgX5K0QG"
   },
   "outputs": [],
   "source": [
    "def save_progress(filepath, data):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_progress(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint {filepath}: {e}\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UubKCIP2K3mX"
   },
   "outputs": [],
   "source": [
    "def extract_landmarks_from_frames(frames):\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    landmark_list = []\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        for frame in frames:\n",
    "            results = face_mesh.process(frame)\n",
    "            if results.multi_face_landmarks:\n",
    "                \n",
    "                face_landmarks = results.multi_face_landmarks[0]\n",
    "                landmarks = [(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark]  \n",
    "                landmark_list.append(landmarks)\n",
    "            else:\n",
    "                landmark_list.append([(0, 0, 0)] * 468)\n",
    "\n",
    "    return landmark_list\n",
    "\n",
    "def extract_landmark_features_with_checkpoint(video_frames_path, checkpoint_path, total_videos):\n",
    "    saved_progress = load_progress(checkpoint_path)\n",
    "    start_index = len(saved_progress)\n",
    "\n",
    "    if start_index >= total_videos:\n",
    "        print(f\"All videos already processed for {checkpoint_path}\")\n",
    "        return np.array(saved_progress)\n",
    "\n",
    "    progress_bar = tqdm(total=total_videos, desc=\"Extracting Landmark Features\")\n",
    "    progress_bar.update(start_index)\n",
    "\n",
    "    for video_idx in range(start_index, total_videos):\n",
    "        video_path = video_frames_path[video_idx]\n",
    "        try:\n",
    "            frames = load_video_frames(video_path)  \n",
    "            landmarks = extract_landmarks_from_frames(frames)\n",
    "            saved_progress.append(landmarks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_idx}: {e}\")\n",
    "            continue\n",
    "        save_progress(checkpoint_path, saved_progress)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return np.array(saved_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1ZCbVytK7kJ"
   },
   "outputs": [],
   "source": [
    "def load_video_frames(video_path):\n",
    "    return [np.random.rand(224, 224, 3) for _ in range(48)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695506,
     "status": "ok",
     "timestamp": 1733912118123,
     "user": {
      "displayName": "Deepfake Detection",
      "userId": "16853363008574283880"
     },
     "user_tz": -330
    },
    "id": "CU_igPveLA93",
    "outputId": "e4ad486d-59b8-4b9d-b99d-ef0ea28f581f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing real videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Landmark Features: 100%|██████████| 967/967 [05:55<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fake videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Landmark Features: 100%|██████████| 967/967 [05:36<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark feature extraction completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_path = 'drive/MyDrive/final_dataset/features/'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    video_frames_real_path = 'drive/MyDrive/final_dataset/cropped_frames/real/'\n",
    "    video_frames_fake_path = 'drive/MyDrive/final_dataset/cropped_frames/fake/'\n",
    "\n",
    "    real_checkpoint = os.path.join(base_path, 'landmarks_real.pkl')\n",
    "    fake_checkpoint = os.path.join(base_path, 'landmarks_fake.pkl')\n",
    "\n",
    "    real_video_paths = [os.path.join(video_frames_real_path, d) for d in os.listdir(video_frames_real_path)]\n",
    "    fake_video_paths = [os.path.join(video_frames_fake_path, d) for d in os.listdir(video_frames_fake_path)]\n",
    "\n",
    "    total_real_videos = len(real_video_paths)\n",
    "    total_fake_videos = len(fake_video_paths)\n",
    "\n",
    "    print(\"Processing real videos...\")\n",
    "    landmark_features_real = extract_landmark_features_with_checkpoint(real_video_paths, real_checkpoint, total_real_videos)\n",
    "\n",
    "    print(\"Processing fake videos...\")\n",
    "    landmark_features_fake = extract_landmark_features_with_checkpoint(fake_video_paths, fake_checkpoint, total_fake_videos)\n",
    "\n",
    "    print(\"Landmark feature extraction completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNBm7qrGX5IL0ORpABi6/DZ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
