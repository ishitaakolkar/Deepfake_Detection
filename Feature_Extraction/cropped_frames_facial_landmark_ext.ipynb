{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNBm7qrGX5IL0ORpABi6/DZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6GTyTJSKGZG","executionInfo":{"status":"ok","timestamp":1733911251651,"user_tz":-330,"elapsed":27688,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"0e3feee5-b8c9-4fbf-a622-1926247fb744"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIpzre7CKmj5","executionInfo":{"status":"ok","timestamp":1733911290573,"user_tz":-330,"elapsed":2936,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"b8e8a41d-d0c3-46f2-f8fe-6c87cf73afc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pickle\n","from tqdm import tqdm\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten"],"metadata":{"id":"kpHxSuCNKlD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPU setup with minimal memory growth and precision\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    for device in physical_devices:\n","        try:\n","            tf.config.experimental.set_memory_growth(device, True)\n","            # Set a memory limit to reduce load\n","            tf.config.set_logical_device_configuration(\n","                device, [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # Example: 4GB\n","            )\n","            print(f\"GPU configured with a memory limit of 4096 MB.\")\n","        except Exception as e:\n","            print(f\"Error configuring GPU: {e}\")\n","else:\n","    print(\"No GPU detected, running on CPU.\")\n","\n","try:\n","    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n","    print(\"Mixed precision enabled for speedup.\")\n","except ValueError:\n","    print(\"Mixed precision not supported, running with default precision.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8B0wcN2KrEx","executionInfo":{"status":"ok","timestamp":1733911315737,"user_tz":-330,"elapsed":554,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"7833a07c-dfe3-4801-8346-7dfdf8bacde3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU configured with a memory limit of 4096 MB.\n","Mixed precision enabled for speedup.\n"]}]},{"cell_type":"code","source":["# Function to save progress\n","def save_progress(filepath, data):\n","    with open(filepath, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","# Function to load progress\n","def load_progress(filepath):\n","    if os.path.exists(filepath):\n","        try:\n","            with open(filepath, 'rb') as f:\n","                return pickle.load(f)\n","        except Exception as e:\n","            print(f\"Error loading checkpoint {filepath}: {e}\")\n","    return []\n"],"metadata":{"id":"RNJaKgX5K0QG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to load frames and extract landmarks\n","# Placeholder for actual landmark extraction (use a library like Dlib or Mediapipe in practice)\n","def extract_landmarks_from_frames(frames):\n","    mp_face_mesh = mp.solutions.face_mesh\n","    landmark_list = []\n","\n","    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n","        for frame in frames:\n","            results = face_mesh.process(frame)\n","            if results.multi_face_landmarks:\n","                # Extract 468 3D landmarks for the detected face\n","                face_landmarks = results.multi_face_landmarks[0]\n","                landmarks = [(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark]  # Normalize to 3D\n","                landmark_list.append(landmarks)\n","            else:\n","                # Append None or zeros if no face is detected\n","                landmark_list.append([(0, 0, 0)] * 468)\n","\n","    return landmark_list\n","\n","# Function to process videos and save landmarks with checkpoints\n","def extract_landmark_features_with_checkpoint(video_frames_path, checkpoint_path, total_videos):\n","    saved_progress = load_progress(checkpoint_path)\n","    start_index = len(saved_progress)\n","\n","    if start_index >= total_videos:\n","        print(f\"All videos already processed for {checkpoint_path}\")\n","        return np.array(saved_progress)\n","\n","    progress_bar = tqdm(total=total_videos, desc=\"Extracting Landmark Features\")\n","    progress_bar.update(start_index)\n","\n","    for video_idx in range(start_index, total_videos):\n","        video_path = video_frames_path[video_idx]\n","        try:\n","            frames = load_video_frames(video_path)  # Function to load frames from a video\n","            landmarks = extract_landmarks_from_frames(frames)\n","            saved_progress.append(landmarks)\n","        except Exception as e:\n","            print(f\"Error processing video {video_idx}: {e}\")\n","            continue\n","        save_progress(checkpoint_path, saved_progress)\n","        progress_bar.update(1)\n","\n","    progress_bar.close()\n","    return np.array(saved_progress)"],"metadata":{"id":"UubKCIP2K3mX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to load video frames (replace this with actual logic for your dataset)\n","def load_video_frames(video_path):\n","    # Dummy example: Replace this with code to load frames from videos\n","    return [np.random.rand(224, 224, 3) for _ in range(48)]  # Simulated 30 frames per video"],"metadata":{"id":"H1ZCbVytK7kJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Define paths and directories\n","    base_path = 'drive/MyDrive/final_dataset/features/'\n","    os.makedirs(base_path, exist_ok=True)\n","\n","    video_frames_real_path = 'drive/MyDrive/final_dataset/cropped_frames/real/'\n","    video_frames_fake_path = 'drive/MyDrive/final_dataset/cropped_frames/fake/'\n","\n","    real_checkpoint = os.path.join(base_path, 'landmarks_real.pkl')\n","    fake_checkpoint = os.path.join(base_path, 'landmarks_fake.pkl')\n","\n","    # Get list of video frame directories\n","    real_video_paths = [os.path.join(video_frames_real_path, d) for d in os.listdir(video_frames_real_path)]\n","    fake_video_paths = [os.path.join(video_frames_fake_path, d) for d in os.listdir(video_frames_fake_path)]\n","\n","    total_real_videos = len(real_video_paths)\n","    total_fake_videos = len(fake_video_paths)\n","\n","    # Extract landmark features\n","    print(\"Processing real videos...\")\n","    landmark_features_real = extract_landmark_features_with_checkpoint(real_video_paths, real_checkpoint, total_real_videos)\n","\n","    print(\"Processing fake videos...\")\n","    landmark_features_fake = extract_landmark_features_with_checkpoint(fake_video_paths, fake_checkpoint, total_fake_videos)\n","\n","    print(\"Landmark feature extraction completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CU_igPveLA93","executionInfo":{"status":"ok","timestamp":1733912118123,"user_tz":-330,"elapsed":695506,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"e4ad486d-59b8-4b9d-b99d-ef0ea28f581f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing real videos...\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Landmark Features: 100%|██████████| 967/967 [05:55<00:00,  2.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing fake videos...\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Landmark Features: 100%|██████████| 967/967 [05:36<00:00,  2.88it/s]"]},{"output_type":"stream","name":"stdout","text":["Landmark feature extraction completed.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}